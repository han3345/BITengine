# 中期报告
两个人采用了两个技术栈：scrapy和传统的requests+beautifulsoup
爬虫的请求头写一下 useragent，就不容易被封了
ignore robots.txt
爬的时候可以限定domain：netloc。不然会爬到别的域名底下
二级域名和本域名下的树形结构。mail.bit.edu.cn    www.bit.edu.cn/xww
从bitren。com获得二级域名 再手动添加一些。
死链的处理方法
需要登陆的网站
限制递归深度
css选择器和xpath
中文编码
结巴分词+lunr前端
绝对路径 相对路径

## 以后可以改进的
如果一个url是一个文档，那么可以搜索文档中的内容
分析一段数字是电话号码
跟进变化，如新闻、教务处通知

2167个html文件，大小为63.1MB。平均每个29k。